We sincerely appreciate you for your recognition of our work and constructive suggestions.



**Question 1:** Can the authors improve their discussion on the Gaussian assumption and how it holds under different aggregation functions for message passing? 

**Answer:** Thank you for your insightful questions. We will answer your questions from two perspectives. Firstly, we examine whether the representations obtained under different aggregation strategies, such as maxing aggregation, still conform to Gaussian distributions. Secondly, we investigate the effectiveness of our approach in cases where the representations deviate from Gaussian distributions.

(1) Here, we examine the normality of representations under various aggregation methods. In Figure 2 of the PDF for rebuttal, we visualize the histograms of node representations under various aggregations, including maxing aggregation, mean aggregation, and no aggregation (e.g., adopting MLP as the encoder). We can observe that under different aggregation strategies, the representations generally exhibit a Gaussian-like appearance.

(2) Furthermore, we explore whether the performance of our method can still hold on when the representations deviate from Gaussian distribution. To this end, the following strategy is adopted in the training process to make node representations deviate from Gaussian distributions. For a representation matrix $\mathbf{H} \in \mathbb{R}^{N \times d}$ from view A or B, we calculate mean $u_i$ and variance $v_i$ of $N$ points from the $i$-th dimension. Then, we can construct a 1-dim Gaussian distribution $p_i(x)$ based on $u_i$ and $v_i$. Further, we calculate the product of probabilities of $N$ samples in the $i$-th dim from $p_i(x)$, $\prod_{k=1}^{N} p_i(H_{ki})$. In practice, we utilize its logarithm $\sum_{k=1}^{N} \log p_i(H_{ki})$. Considering all dimensions, we construct a loss $\mathcal{L}_{dev} = \sum_{i=1}^{d} \sum_{k=1}^{N} \log p_i(H_{ki})$. We can minimize $\mathcal{L}_{dev}$ to make representations deviate from Gaussian distribution. It can be realized by attaching $\alpha \cdot \mathcal{L}_{dev}$ to the original loss, where $\alpha$ controls the degree of deviation. The curves between $\alpha$ and performance are placed in Figure 3 of the PDF for rebuttal. Overall, with the increase of $\alpha$, the performance decreases. Besides, it is not sensitive to small deviations. In other words, when node representations deviate slightly from Gaussian distribution, the performance of our method can still be well maintained. This appears to be good news. In this circumstance, our method allows representations to deviate from normality, which grants the method higher robustness and broader application scenarios. In summary, although our method is derived and proposed under the Gaussian assumption, it remains effective even when the representations deviate moderately from Gaussian distribution. The above discussion seems to offer a potential idea for extending our research. For common distributions such as Gamma distribution and Beta distribution, their mutual information may be directly calculated from empirical data. We can assume that the representations follow such a distribution and design a loss function (e.g., the negative of $\mathcal{L}_{dev}$) to constrain the representations to approach the assumed distribution, where $\mathcal{L}_{dev}$ is tailored for the assumed distribution.





Thank you very much for your recognition and positive comments on our work. The reviewer mcA5 mentioned a paper that shares some similarities with ours. After carefully reading this paper, we provided a detailed response to reviewer mcA5 outlining the differences and our contributions beyond this work. I sincerely recommend you to take your valuable time to read that paper, and then reevaluate the contributions of our work. If you think that our contributions are limited due to the presence of that work and can't match up to your high appraisal, please feel free to adjust your rating accordingly. We genuinely welcome your valuable insights. 

The above discussion with you has been a rewarding academic discourse, and we eagerly anticipate further interactions in the future.
